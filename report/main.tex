\documentclass[a4paper, onecolumn, 12pt]{article}
\usepackage[left=33mm,right=33mm,top=35mm,columnsep=15pt]{geometry} 

% basic packages
\usepackage[english]{babel} %language
\usepackage[utf8]{inputenc} %input encoding
\usepackage{float} %position of floating objects
\usepackage{bookmark} %hyperlinks in pdf
\usepackage{subcaption}
\usepackage[T1]{fontenc}
\usepackage{lipsum} %placeholder text

% math packages
\usepackage{amsthm} %theorems
\usepackage{amsmath} %math
\usepackage{mathtools} %still math

% code packages
\usepackage{listings} %code
\usepackage{xcolor} %syntax highlighting
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fancyvrb}

% todonotes package
\usepackage{xkeyval}
\usepackage{tikz}
\usepackage{calc}
\setlength {\marginparwidth }{2cm}
\usepackage{todonotes} % Load the todonotes package

% custom commands
\newcommand\tab[1][.3cm]{\hspace*{#1}}
\newcommand\tabeq[1][.5cm]{\hspace*{#1}}
\newcommand\suggestion[1]{\textcolor{violet}{\textbf{Suggestion}: #1}}
\newcommand\old[1]{\textcolor{brown}{\textbf{Old}: #1}}
\newcommand\doubt[1]{\textcolor{purple}{#1 \textbf{(?)}}}
\newcommand\code[1]{\textcolor{teal}{\texttt{#1}}}

% \usepackage{physics}
% \usepackage{adjustbox}
% \usepackage{placeins}
% \usepackage{csquotes}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}

\title{Remote Interaction with a Nao Humanoid in competitive games \\ Elective in AI / HRI Report}
\author{Flavio Maiorana \and Valerio Spagnoli \and Flavio Volpi}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}
In the RoboCup games, robots are fully autonomous, yet there is potential for 
improvement through human interaction. Just as human soccer players benefit from 
receiving real-time suggestions or explicit instructions during a match, 
autonomous soccer robots could also enhance their performance by incorporating 
informed guidance. 
\old{In this project, we aim to make a small step in that direction by developing a 
system that allows a human operator to remotely control and interact with a Nao 
Humanoid through a graphical interface.}
\suggestion{In this project, we aim to take a step in this direction by developing a 
system that enables a human operator to interact with a Nao humanoid robot via a graphical 
interface and voice commands. The goal is to enhance the robot's performance and 
provide real-time strategic suggestions, similar to the role of a football coach.}

\subsection{Context and Motivation}
\label{sec:context}

\subsubsection{RoboCup SPL Challenge}
This project was specifically designed to be used in the RoboCup 2024 SPL challenge, 
where two robots of one team had to compete against two robots of the opponent team, 
and one of the two robots for each team was controlled by a human operator. Furthermore, 
the rules of the challenge forced the human operator to turn his back to the field, 
in order to not directly observe the environment. This constrained us to make use of the
directional robot-human communication also for the reconstruction of the world model.

\subsubsection{Possible Extensions}  
The system developed in this project could be adapted to various other contexts. For instance, 
it could be applied to the RoboCup SPL main competition, where a human operator for each team 
might be allowed to provide real-time instructions to the robots via voice commands, using the 
graphical interface to view the reconstructed world model of the entire team.  

Another potential extension involves modifying the system to enable robots to play alongside human players. 
In this scenario, the robot would need to interpret human commands and execute them in real-time, allowing 
for mixed teams of humans and robots in a soccer game.

\subsection{Objectives}
\label{sec:obj}
\todo[inline]{Ci sono troppe ripetizioni di questa frase, alcune vanno tolte}
In the context of the RoboCup SPL challenge, the main objective of the project is to develop a framework
that allows the human operator to use the robot as a \textit{proxy} to interact with the environment, namely the soccer
field. 
To do this, a form of bidirectional interaction between the human operator and the controlled robot is necessary. 

\subsubsection{Bidirectional Communication}  
Receiving instructions from a coach can significantly impact the outcome of a soccer match. 
Likewise, getting feedback from the robot when issuing commands greatly enhances 
the quality of the interaction. The human acting as a coach must be completely aware of the robot's surroundings,
while the robot must be able to interpret the human's commands and respond accordingly. 
\begin{itemize}  
    \item \textbf{Robot-to-Human Communication}: The robot provides the human operator with 
    all the necessary information to reconstruct the world model. These data are transmitted 
    over the network to the operator's computer, where are filtered and displayed on the 
    graphical interface. Moreover, the robot responds with vocal feedback to confirm received 
    commands \doubt{or to report any issues (e.g. the impossibility to execute the command)}.  
    \item \textbf{Human-to-Robot Communication}:  
    \begin{itemize}  
        \item \textbf{Coach-to-Robot Communication}: The human operator, acting as a coach, can 
        analyze the reconstructed world model using the graphical interface and decide to send commands to the robot. 
        These commands are transmitted through the network or via voice commands.  
        \item \suggestion{\textbf{Referee-to-Robot Communication}: In the context of the RoboCup 
        SPL challenge, another human, acting as a referee, can send commands to the robot using 
        whistle signals. The robot must be capable of interpreting these signals and responding 
        accordingly.}  
    \end{itemize}  
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{assets/bidirectional-communication.png}
    \label{fig:bidirectional-communication}
    \caption{Bidirectional communication scheme}
\end{figure}


\subsubsection{Real-time Interaction}
In the context of the RoboCup SPL challenge, the system must process commands in real-time, given the 
competitive nature of the scenario. The robot needs to promptly interpret and execute the human operator's 
commands while providing immediate feedback on the execution status. Any delay in processing could adversely 
affect the robot's performance and the outcome of the match. Even in other applications, real-time interaction 
is critical to maintain the robot's responsiveness to human input and ensure accurate reconstruction of the robot's world model.

\subsection{Summary of the results}
\label{sec:summary}

Our system achieved the objective of allowing a human operator to control a Nao
by voice and using a graphical interface, and to receive feedback from the robot
in response to the commands. The system integrates various features, such as:
\begin{itemize}
    \item \textbf{communication}: which manages the communication between the robot and
    the human operator
    \item \textbf{mental model}: responsible for representing the field by
    accessing only the robot's perceptions
    \item \textbf{interaction}: by answering to the commands before executing
    them, considering also the feasibility of the execution itself,
    \item \textbf{memory}: the past command is held in memory, in order to
    resume it in case it is needed.
\end{itemize}
The system was tested in the RoboCup 2024 SPL challenge, where SQPR Team reached
the third place, demonstrating the effectiveness of the system in a competitive
environment.

\newpage

\section{Related Work}
\label{sec:rel}

Interpreting human signals has been a challenge for some time now in Robotics.
Humans communicate through various modalities, including vision, audio, and
motion. This multimodal nature provides rich information that sensory inputs can
capture and analyze. 

Recent advances in Deep Learning have facilitated the
integration of multimodal data, significantly improving the comprehension of
relationships within individual modalities, a key factor for precise message
interpretation \cite{LIU20183} \cite{su2023recent}.

In RoboCup Soccer, human-robot interaction is predominantly one-way, with human
referees conveying game states and events to robots. A significant trend in the
RoboCup SPL is the progressive reduction of network communication in
favor of human-like signal interpretation, allowing robots to interpret human
signals more naturally. \cite{digiambattista}

A notable case worth to mention is also \cite{antonioni}, where they propose an
approach to improve the decision making process through the audience noise by
extracting relevant features through MFCC coefficients and applying a
reinforcement learning pipeline. 

This case could fall into a broader category where the goal is to improve the
communication from an ideal coach to the robot in order to improve planning and
decision-making. In particular, \cite{musumeci} tackles this problem by
designing a system that enriches the planning process with temporal goals and
constraints given by human indications. 

Our work is inspired by these studies, and the goal is to develop a system that
allows a human operator to have a one-to-one interaction with a robot, acting
like a coach in a soccer match.

\newpage
\section{Solution}
\label{sec:sol}

The system can be divided in two main parts: the framework backbone, written in
C++, which runs on the robot itself, and the interface, written in Python and
Node.js, responsible for issuing commands to the robot and receiving feedback or
sensory information. 
\\

\suggestion{WRITE IN THIS OTHER WAY}\\
The system can be divided in three parts: 
    \begin{itemize}
        \item framework backbone, written in C++, which runs on the robot itself;
        \item interface in Node.js, where the reconstructed world
        model is displayed and the commands to control the robot are available; 
        \item Python server, which allows the comunication between the robot 
        and the interface, i.e. it is responsible for issuing commands to the robot 
        and receiving feedback or sensory information. 
    \end{itemize}

\subsection{Framework backbone}
The framework on which everything stands on is the one of the SQPR team of Sapienza University \cite{},
which is derived from that one of the
German team B-Human of the year 2021 \cite{bhuman2023}, University of Bremen. At low level, the robot is 
controlled by four threads:
\begin{itemize}
    \item Upper camera thread: it deals with upper camera of the robot, positioned on its forehead;
    \item Lower camera thread: it deals with lower camera of the robot, placed on its chin;
    \item Cognition: it is responsible for collecting all informations from the 
    environment through cameras and sensors; also, using some informations as input,
    it returns high level commands about the actions the robot has to execute;
    \item Motion: it converts high level commands of the thread Cognition in effective
    motion control of the 25 joints of the robot;
\end{itemize}
Both camera threads capture images from the cameras of the robot, and work on them 
to retrieve informations to describe the world.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{assets/threads.png}
    \label{fig:threads}
    \caption{Threads}
\end{figure}

\subsubsection{Representations and Modules}
The two main components for the collection and storage of information from the environment
are called Representations and Modules.
Representations store informations at the current instant, and they represent the actual overview
of the world. Each representation is a derived class of the class $Streamable$, which allows a 
direct connection of all attributes and functions of the given representation with all other
representations and modules. Furthermore, each representation has a single module provider, which
is the only capable of modifying its attributes.

Instead, the task of the modules is to make computations which require specific inputs and returns
determined output. In details, a module can specify the representations it needs in input through
the macros REQUIRES and USES, and must specify the representations it is going to modify through
the macro PROVIDES. A module must specify a function named $update$, that has the scope to 
perform the real updates of the informations inside the provided representations.

The correct usage of REQUIRES and USES is established by a scheduler inside the software, which decides 
the right execution order of the modules according to cyclic dependencies between representations.
For example, if a module M1 updates the representation R1, and this representation is required from 
module M2 to modify its representation, the module M2 cannot be executed before the module M1, so the
scheduler will execute in order M1 and M2. 
\todo[inline]{aggiungere immagine di due moduli che richiedono rappresentazioni}
Every update function has an execution frequency of 83 Hz, which practically allows an information 
refresh in real time.

\subsubsection{Behaviors and Skills\&Card}
Behaviors represent the real actions the robot will take during the game. They are modeled as finite 
automata, according to CABSL standard ($C$-$based$ $Agent$ $Behavior$ $Specification$ $Language$) \cite{}.
This is based on the concepts of options, states, transitions and actions: the options are finite-state machines that
describe a determined way of acting; transitions define the conditions to move between two states of the graph; 
when the latter arrives in a state, the specified actions are executed.
Actions are expressed as $Skills$, which are the leaves of the graph and the lowest abstraction elements.
They make calls directly to the motion engine of the robot.

Option idea matches with the concept of Card: each card represents one option, and it is the interpretation
of an higher abstraction level for the actions. 
To enter in a card, a robot has to satisfy its specified preconditions; to exit from a card, preconditions are not
satisfied anymore or its postconditions are met instead.
Cards are the fundamental elements that compose the Decks, which are literally ordered collections of cards.
The order establishes the priority of the cards: the ones on the top have higher priority than the ones on the bottom.
Starting from the first card, the agent enters in it if its preconditions are satisfied, otherwise he discards 
the card and checks the second one, and so on. So it is very important to establish the right disposition,
according to own puproses. 

This hierarchy of the system allows to link the decks to the roles of the players in the field. 
In this way it is possible to associate just the cards relative to the corresponding role, avoiding
possible undesired behaviors. 


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{assets/flowchart.png}
    \label{fig:flowchart}
    \caption{Complete pipeline of the game states}
\end{figure}

\subsubsection{Memory}
\todo[inline]{Boh}
\suggestion{
    The robot's perception is not limited to the current state of the environment; 
    it also utilizes past information to inform its decisions. The robot maintains 
    a memory of past events and when they occurred. For instance, it can remember the 
    last time it saw the ball or when it last encountered an obstacle. This historical 
    data is crucial for the human operator to accurately reconstruct the world model 
    and make informed decisions. Additionally, the robot can recall the last command 
    it received and resume it if necessary.
}

\subsubsection{Social Reasoning}

An important aspect of RoboCup SPL games are the game rules. These are conceived
from year to year to shift the games to be more realistic. If we want, they
could be considered as a form of social rules which the robots need to obey to
while playing. Depending on the game state, some actions are not permitted. For
instance, in SET the robots are not allowed to move. Similarly, one could impose
a rule that avoids the robot to score in its own goal, even if it is requested
to. Of course, in order to to some adequate social reasoning, it is necessary
for the robot to have a sufficiently detailed model of the game state and the
field.

\subsubsection{Mental Modeling}

\todo[inline]{Come rappresenta il mondo il robot? Stato del gioco e del campo fisico}
Every robot has two different mental models, and they represent the way the robot 
describes the world: a local world model and a global world model.
The former is reconstructed starting just from the preceptions the robot itself has,
so the generated mental model will be partial. Furthermore, the model is expressed 
with respect to the robot itself.
It contains:
\begin{itemize}
    \item relative positions of the perceived obstacles (also teammates and opponents)
    \item relative positions of the perceived landmarks
    \item relative position of the ball
    \item relative velocity of the ball
    \item time when ball was seen last time
    \item self localization position in the field
\end{itemize}

The latter instead is reconstructed using the local world model and the information
received from the teammates through the network. Because the local perceptions of the 
single robot can be not so accurate, due to some additional noise that affects the sensors,
some filters are applied to the single attributes that constitute the world model. This 
is done to result in a more accurate probabilistic estimate of the values of those attributes.
It is composed by:
\begin{itemize}
    \item global positions of the obstacles (teammates and opponents)
    \item global positions of the landmarks
    \item global position of the ball
    \item global velocity of the ball
    \item time when ball was seen last time from some teammates
\end{itemize}

Cohexistance of both the mental models is very important.
The global one takes advantage of the intrinsic properties of a 
distributed system, e.g. the capability to capture simultaneously different
aspects and glimpses of the environment; this is very helpful to have a wider
knowledge of the world, but it is affected by possible noisy informations coming 
from robots that have not so precise informations.
On the other hand, the local model is the key of the interaction between the robot and
the environment, because it is a more accurate image of its surroundings.
Indeed, it gives a smaller but deeper representation of the world.

\subsection{Python server}
\todo[inline]{questa in realtà è la roba che sta in implementation,
però secondo me avrebbe senso metterci qualcosa, anche per far capire
che le cose visualizzate nell'interfaccia si ottengono tramite questo}

\subsection{Interface}

We opted for a solution that prioritizes high-level commands and audio-visual
feedback. The graphical interface is made by a bunch of buttons, that allow the
human operator to send commands to the robot, and a 2D representation of the
field that shows the robot's position and its reconstruction of the world.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{assets/interface.png}
    \caption{The graphical interface}
    \label{fig:interface}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{assets/simrobot.png}
    \caption{The Corresponding Field Configuration}
    \label{fig:nao}
\end{figure}

\newpage
\section{Implementation}
\label{sec:impl}

From an implementative point of view, the system can be divided in four modules: 
\begin{itemize}
    \item \textbf{Communication Layer}: exchanges messages between the robot and the human operator
    \item \textbf{Framework}: the backbone of the system, which runs on the robot
    \item \textbf{GUI}: the graphical interface that allows the human operator to interact with the robot
    \item \textbf{Speech-to-Text Module}: the module that converts the human operator's voice commands to text
\end{itemize}

These modules are wrapped around the two main data structs that encapsulate the
exchanged messages, namely the \textit{HumanCommand} struct and the
\textit{DebugInfo} struct. The former is used to send commands from the human
operator to the robot. It has the following structure:
\begin{verbatim}
    1B: Command body
    1B: Command head
    1B: Strategy
    8B: [pos_x, pos_y]
    command_format: "<BBBff"
\end{verbatim}

Basically, it is formatted as a tuple of 5 elements, where the first three are
Bytes and the last two are floats. The last two elements are the coordinates of
the target position, which are indicated on the 2D field. This precise structure
is needed to be able to send the command through the socket and parse it on the
C++ side.

The \textit{DebugInfo} struct, on the other hand, is used to send feedback from
the robot to the human operator. It has the following structure:

\begin{verbatim}
1)      4B header
2)      1B version
3)      1B player_num
4)      1B team_num
5)      1B fallen
6-9)    3f (12B) [pos_x, pos_y, theta]
9)      1f (4B) ball_age
10-12)  2f (8B) [ball_pos_x, ball_pos_y]
12)     1H (2B) num_of_data_bytes
13)     1B player_role
14)     1B current_obs_size
15-35)  20B obstacle_types  
35-55)  20f (80B) obs_center_x
55-75)  20f (80B) obs_center_y
75-95)  20f (80B) obs_last_seen
95)     1H (2B) message_budget
96)     1H (2B) secs_remaining
97-99)  2B arm_contact
99-101) 2B arm_push_direction
101-103)2f (8B) arm_time_of_last_contact
103)    2f (8B) padding (whistle)
104-106) 2f (8B) teamball
106-108) 2f (8B) teamballvel
108)    12B padding

data_format: "<d4sBBBB3ff2fHBB20B20f20f20fHH2B2B2f2f2f2f12B"
\end{verbatim}

This data packet contains all the information that is needed to have a graphical
representation of the robot's mental model of the field.

The most important fields are:
\begin{itemize}
    \item \code{fallen}: a boolean that indicates if the robot is fallen,
    \item \code{pos\_x, pos\_y, theta}: the position of the robot in the field,
    \item \code{ball\_age}: the age of the ball that indicates how many cycles have passed since the robot saw the ball,
    \item \code{ball\_pos\_x, ball\_pos\_y}: the position of the ball in the field,
    \item \code{obstacle\_types}: an array that indicates the type of the obstacles in the field (e.g. teammate, opponent),
    \item \code{obs\_center\_x, obs\_center\_y}: the position of the obstacles in the field,
    \item \code{obs\_last\_seen}: the time when the obstacles were last seen,
    \item \code{secs\_remaining}: the time remaining in the game,
    \item \code{arm\_contact}: a boolean that indicates if the robot is in contact with an obstacle,
    \item \code{arm\_push\_direction}: the direction in which the robot arm is pushed,
    \item \code{arm\_time\_of\_last\_contact}: the time when the robot was last in contact with an obstacle,
    \item \code{teamball, teamballvel}: the position and velocity of the ball in the field.
\end{itemize}
Using these informations, the human operator can have a detailed view of the field through the robot's perceptions.

\subsection{Communication Layer}

The communication layer is responsible for the exchange of messages between the
robot and the human operator. It is written in Python and based on UDP sockets. 

\begin{verbatim}
def send_command_to_cpp(self, command: int, strategy: int, x: int, y: int):
    if command_number > self.config.command_offset:
        command_body_number = Command.Null.value
        command_head_number = command_number - self.config.command_offset
    else:
        command_body_number = command_number
        command_head_number = Command.Null.value
    encoded_data = struct.pack(
        self.config.command_format,
        command_body_number,
        command_head_number,
        strategy_number,
        x_position,
        y_position
    )
    try:
        client_socket.sendto(encoded_data, (robot_ip, command_send_port))
        print(f"Sending message to C++: {encoded_data}")
    except Exception as e:
        print(f"Error in send_message: {e}")
\end{verbatim}

This function sends a command that is received from the GUI to the robot. The
receive function from nodejs is the following:

\begin{verbatim}
def receive_command_from_js(self) -> tuple[int, int, int, int]:
    try:
        data, addr = self.server_socket.recvfrom(1024)
    except Exception as e:
        print(f"Error in receiving the message: {e}")
    try:
        message = data.decode()
    except UnicodeDecodeError:
        print(f"Received non-UTF-8 message from JavaScript: {data} from {addr}")
    message_split = message.split('|')[1]
    content_message = message_split.split(',')
    task_type = content_message[2]
    command_number = Command[task_type].value
    strategy_number = int(content_message[5])
    task_label = content_message[4]
    selection = content_message[1]
    print(f"Task Label: {task_label}")
    if selection == "selection":
        x_position = int(content_message[6])
        y_position = int(content_message[7])
        print(f"X Position: {x_position}")
        print(f"Y Position: {y_position}")
    else:
        x_position = 0
        y_position = 0
    return command_number, strategy_number, x_position, y_position
\end{verbatim}
Basically, the python scripts receives a command in form of a string from the
GUI, unpacks it, puts it into a serialized format and sends it to the robot.


\subsection{Framework}

The framework is the backbone of the system. The two main functions are the
update of the CommandReceiver and the update of the DebugMessageHandler (which
sends sensory information). The CommandReceiver is responsible for updating the
representation HumanCommand, while giving some audio feedback to the human about
the received command. In the following function, the command is parsed,
converted to enums and stored in the HumanCommand representation. 

\begin{verbatim}
void CommandReceiver::update(HumanCommand& command) {
  
  if (theRobotInfo.number != RobotInfo::RoleNumber::controlled) return;
  
  char buffer[BUFFER_SIZE];
  int n = socket_read.read(buffer, BUFFER_SIZE);
  
  // First field (command_body): unsigned char
  HumanCommand::CommandBody received_command_body = 
      static_cast<HumanCommand::CommandBody>(buffer[0]);
  
  // Second field (command_head): unsigned char
  HumanCommand::CommandHead received_command_head = 
      static_cast<HumanCommand::CommandHead>(buffer[1]);
  
  // Third field (strategy): unsigned char
  HumanCommand::Strategy strategy = 
      static_cast<HumanCommand::Strategy>(buffer[2]);
  
  // Fourth field (x_pos): int (4 bytes)
  float x_pos;
  std::memcpy(&x_pos, buffer + 3, sizeof(x_pos));
  
  // Fifth field (y_pos): int (4 bytes)
  float y_pos;
  std::memcpy(&y_pos, buffer + 7, sizeof(y_pos));
  
  if (n > 0) {
      SystemCall::say(HumanCommand::CommandBody2String(received_command_body));
  
      if(received_command_body != HumanCommand::CommandBody::BaseCommandBody){
          command.commandBody = received_command_body;
          command.x = x_pos;
          command.y = y_pos;
      }
      if(received_command_head != HumanCommand::CommandHead::BaseCommandHead)
          command.commandHead = received_command_head;
  
      command.strategy = strategy;
  }
  return;
}
\end{verbatim}

Once the command is stored in the HumanCommand representation, it is used by the
specific behavior we designed for the interacting robot. The update function
iterates over the possible commands (defined as an enumeration) in the start
state with a switch, executing the corresponding skill for each command. For
instance:

\begin{verbatim}
state(pass)
{
    transition
    {
    if (theHumanCommand.commandBody != HumanCommand::CommandBody::Pass)
        goto start;
    }

    action
    {
    int num = theTeamData.numberOfActiveTeammates;
    int size = theTeamData.teammates.size();
    if(num > 0){
        theSaySkill("Pass the ball");
        Vector2f target = theTeamData.teammates[0].theRobotPose.translation;
        KickInfo::KickType kickType = theLibPass.getKickType(target);
        float distance = theLibMisc.distance(target, theRobotPose);
        theGoToBallAndKickSkill(calcAngleToTarget(target), 
            kickType, true, distance);
    }
    else{
        theSaySkill("Go to ball and dribble");
        theGoToBallAndDribbleSkill(calcAngleToTarget(
            theLibStriker.strikerMovementPoint()));
    }
    }
}
\end{verbatim}

Whenever a new command is received, the robot answers with an audio feedback and
reiterates over the command list. 


\subsection{Speech-to-Text Module}

\todo[inline]{Blablabla}

\subsection{Interface}


\section{Results}
\label{sec:res}
\subsection{Usability}  
The system was tested exclusively by team members, and the feedback was positive 
across various aspects:  
\begin{itemize}  
    \item \textbf{Reliability}: The system underwent extensive testing before the RoboCup 
    SPL challenge and was used during the competition without any significant issues. 
    Communication between the robot and the human operator remained stable, with the 
    robot accurately executing commands and providing the expected feedback.  
    \item \textbf{Validity}: The robot successfully interpreted the human operator's 
    commands and executed them in real-time. Additionally, it provided the operator with a comprehensive view of the field through its sensors and offered feedback on the execution of commands.  
    \item \textbf{Sensitivity}: The system is sensitive to network delays, which can 
    impact the real-time interaction between the robot and the human operator.  
\end{itemize}  

The graphical interface was designed to be user-friendly and intuitive, ensuring that 
the human operator can interact with the robot quickly, which is essential in the 
competitive RoboCup SPL challenge, where each match lasts only 3 minutes.  

\subsection{Adversarial Environment - RoboCup 2024 SPL Challenge}  
The system was tested in the highly competitive and adversarial setting of the RoboCup 
2024 SPL challenge. Several factors in this environment could impact the system’s 
performance, such as:  
\begin{itemize}  
    \item \textbf{Network Delays}: The system’s real-time interaction can be affected 
    by network delays, posing challenges to command execution.  
    \item \textbf{Game Rules}: The robot must be capable of interpreting and adhering 
    to game rules in real-time.  
    \item \textbf{Opponent's Strategy}: Opponents aim to exploit any weaknesses in the 
    system, requiring the robot and the human-operator to adapt to their strategies dynamically.  
\end{itemize}  
Despite these challenges, the system performed effectively, and the team secured third 
place in the competition.  


\section{Experimental Evaluation} 

\section{Conclusion}
\label{sec:con}

\todo[inline]{Amen}

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
