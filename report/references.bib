@InProceedings{digiambattista,
author="Di Giambattista, Valerio
and Fawakherji, Mulham
and Suriani, Vincenzo
and Bloisi, Domenico D.
and Nardi, Daniele",
editor="Chalup, Stephan
and Niemueller, Tim
and Suthakorn, Jackrit
and Williams, Mary-Anne",
title="On Field Gesture-Based Robot-to-Robot Communication with NAO Soccer Players",
booktitle="RoboCup 2019: Robot World Cup XXIII",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="367--375",
abstract="Gesture-based communication is commonly used by soccer players during matches to exchange information with teammates. Among the possible forms of gesture-based interaction, hand signals are the most used. In this paper, we present a deep learning method for recognizing robot-to-robot hand signals exchanged during a soccer game. A neural network for estimating human body, face, hands, and foot position has been adapted for the application in the robot soccer scenario. Quantitative experiments carried out on NAO V6 robots demonstrate the effectiveness of the proposed approach. Source code and data used in this work are made publicly available for the community.",
isbn="978-3-030-35699-6"
}

@InProceedings{antonioni,
author="Antonioni, Emanuele
and Suriani, Vincenzo
and Solimando, Filippo
and Nardi, Daniele
and Bloisi, Domenico D.",
editor="Alami, Rachid
and Biswas, Joydeep
and Cakmak, Maya
and Obst, Oliver",
title="Learning from the Crowd: Improving the Decision Making Process in Robot Soccer Using the Audience Noise",
booktitle="RoboCup 2021: Robot World Cup XXIV",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="153--164",
abstract="Fan input and support is an important component 
in many individual and team sports, ranging from athletics to 
basketball. Audience interaction provides a consistent impact 
on the athletes' performance. The analysis of the crowd 
noise can provide a global indication on the ongoing game 
situation, less conditioned by subjective factors that 
can influence a single fan. In this work, we exploit 
the collective intelligence of the audience of a robot 
soccer match to improve the performance of the robot 
players. In particular, audio features extracted from 
the crowd noiseare used in a Reinforcement Learning 
process to possibly modify the game strategy. 
The effectiveness of the proposed approach is 
demonstrated by experiments on registered crowd 
noise samples from several past RoboCup SPL matches.",
isbn="978-3-030-98682-7"
}

@InProceedings{musumeci,
author="Musumeci, Emanuele
and Suriani, Vincenzo
and Antonioni, Emanuele
and Nardi, Daniele
and Bloisi, Domenico D.",
editor="Eguchi, Amy
and Lau, Nuno
and Paetzel-Pr{\"u}smann, Maike
and Wanichanon, Thanapat",
title="Adaptive Team Behavior Planning Using Human Coach Commands",
booktitle="RoboCup 2022: Robot World Cup XXV",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="112--123",
abstract="In its operating life, an agent that needs to act 
in real environments is required to deal with rules and 
constraints that humans ask to satisfy. The set of 
rules specified by the human might influence the role 
of the agent without changing its goal or its current
task. To this end, classical planning methodologies 
can be enriched with temporal goals and constraints
that enforce non-Markovian properties on past traces. 
This work aims at exploring the application of 
real-time dynamic generation of policies whose possible
trajectories are compliant with a set of Pure-Past
Linear Time Logic rules, introducing novel human-robot 
interaction modalities for the high-level control 
of strategies for multiple agents. For proving the 
effectiveness of the proposed approach, we have
carried out an evaluation on a partially observable, 
unpredictable, and dynamic scenario: the RoboCup soccer 
competition. In particular, we exploit human indications 
to condition the robot's behavior before or during the time 
of the match, as happens during human soccer matches.",
isbn="978-3-031-28469-4"
}

@article{su2023recent,
  title={Recent advancements in multimodal human--robot interaction},
  author={Su, Hang and Qi, Wen and Chen, Jiahao and Yang, Chenguang and Sandoval, Juan and Laribi, Med Amine},
  journal={Frontiers in Neurorobotics},
  volume={17},
  pages={1084000},
  year={2023},
  publisher={Frontiers Media SA}
}


@article{LIU20183,
title = {Deep Learning-based Multimodal Control Interface for Human-Robot Collaboration},
journal = {Procedia CIRP},
volume = {72},
pages = {3-8},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.224},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303846},
author = {Hongyi Liu and Tongtong Fang and Tianyu Zhou and Yuquan Wang and Lihui Wang},
keywords = {Human-robot collaboration, Deep learning, Robot control},
abstract = {In human-robot collaborative manufacturing, industrial robot is required to dynamically change its pre-programmed tasks and collaborate with human operators at the same workstation. However, traditional industrial robot is controlled by pre-programmed control codes, which cannot support the emerging needs of human-robot collaboration. In response to the request, this research explored a deep learning-based multimodal robot control interface for human-robot collaboration. Three methods were integrated into the multimodal interface, including voice recognition, hand motion recognition, and body posture recognition. Deep learning was adopted as the algorithm for classification and recognition. Human-robot collaboration specific datasets were collected to support the deep learning algorithm. The result presented at the end of the paper shows the potential to adopt deep learning in human-robot collaboration systems.}
}

@Misc{bhuman2023,
  author = {Thomas R{\"o}fer and Tim Laue and Fynn B{\"o}se and Arne Hasselbring and Jo Lienhoop and Lukas Malte Monnerjahn and Philip Reichenberg and Sina Schreiber},
  title = {{B}-{H}uman Code Release Documentation 2023},
  year = {2023},
  note = {Only available online: \url{https://docs.b-human.de/coderelease2023/}}
}

@Misc{blocco6,
  author = {Luca Iocchi},
  title = {{Lecture 6} â€“ {User Evaluation}},
  note = {Human-Robot Interaction Lecture Slides, 2024},
}

@Misc{spqr,
  author = {SPQR-team, Sapienza University of Rome},
  title = {SPQR-team released code 2023},
  year = {2023},
  note = {Only available online: \url{https://github.com/SPQRTeam/spqr2023.git}}
}
@InProceedings{CABSL,
  author="R{\"o}fer, Thomas",
  editor="Akiyama, Hidehisa
  and Obst, Oliver
  and Sammut, Claude
  and Tonidandel, Flavio",
  title="CABSL -- C-Based Agent Behavior Specification Language",
  booktitle="RoboCup 2017: Robot World Cup XXI",
  year="2018",
  publisher="Springer International Publishing",
  address="Cham",
  pages="135--142",
  abstract="This paper describes the C-based Agent Behavior Specification Language (CABSL) that is available as open source [8]. It allows specifying the behavior of a robot or a software agent in C++11. Semantically, it follows the ideas of the Extensible Agent Behavior Specification Language (XABSL) developed by L{\"o}tzsch et al. [6], i.e. robot behavior is described as a hierarchy of finite state machines. However, its integration into a C++ program requires significantly less programming overhead than when using XABSL. CABSL has been part of all B-Human code releases since 2013 [9], but it is now also available as a standalone release that works without the B-Human base system.",
  isbn="978-3-030-00308-1"
}
